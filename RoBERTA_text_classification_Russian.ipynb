{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pos = pd.read_csv('positive.csv',header=None, sep=\";\")[3][0:2000]\n",
    "text_neg = pd.read_csv('negative.csv',header=None, sep=\";\")[3][0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @first_timee хоть я и школота, но поверь, у на...\n",
      "1    Да, все-таки он немного похож на него. Но мой ...\n",
      "2    RT @KatiaCheh: Ну ты идиотка) я испугалась за ...\n",
      "3    RT @digger2912: \"Кто то в углу сидит и погибае...\n",
      "4    @irina_dyshkant Вот что значит страшилка :D\\nН...\n",
      "Name: 3, dtype: object\n",
      "0    на работе был полный пиддес :| и так каждое за...\n",
      "1    Коллеги сидят рубятся в Urban terror, а я из-з...\n",
      "2    @elina_4post как говорят обещаного три года жд...\n",
      "3    Желаю хорошего полёта и удачной посадки,я буду...\n",
      "4    Обновил за каким-то лешим surf, теперь не рабо...\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_pos.head())\n",
    "print(text_neg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pos = text_pos.to_frame()\n",
    "text_neg = text_neg.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pos[4] = 1\n",
    "text_neg[4] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pos = text_pos.rename(columns={3: 'Data', 4: 'Label'})\n",
    "text_neg = text_neg.rename(columns={3: 'Data', 4: 'Label'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all = pd.concat([text_pos, text_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all = text_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all = text_all.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>жаль, что не написал записки для #helloday зар...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>племянница 14ти лет набила татухи.. куда катит...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Хоть я года три назад и писал микростатью на х...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Совсем скоро НАШ концерт!!! 10 декабря!\"Бочка\"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@lionheartinside иди против них!бунтуй!ложись ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3995</td>\n",
       "      <td>я сметанный колобок:)\\nахахааххаах\\nсегодня вс...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3996</td>\n",
       "      <td>пошли гулять, стало плохо, подруку доводили до...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3997</td>\n",
       "      <td>RT @30Chemical_Day: @Echelonidiot30 пищально( ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3998</td>\n",
       "      <td>Только что рассказывал доклад по истории,про Д...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3999</td>\n",
       "      <td>RT @nika_yavorskaya: а на вулиці снніііііііг*_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Data  Label\n",
       "0     жаль, что не написал записки для #helloday зар...      0\n",
       "1     племянница 14ти лет набила татухи.. куда катит...      0\n",
       "2     Хоть я года три назад и писал микростатью на х...      0\n",
       "3     Совсем скоро НАШ концерт!!! 10 декабря!\"Бочка\"...      1\n",
       "4     @lionheartinside иди против них!бунтуй!ложись ...      1\n",
       "...                                                 ...    ...\n",
       "3995  я сметанный колобок:)\\nахахааххаах\\nсегодня вс...      1\n",
       "3996  пошли гулять, стало плохо, подруку доводили до...      0\n",
       "3997  RT @30Chemical_Day: @Echelonidiot30 пищально( ...      0\n",
       "3998  Только что рассказывал доклад по истории,про Д...      1\n",
       "3999  RT @nika_yavorskaya: а на вулиці снніііііііг*_...      1\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, text_test = train_test_split(text_all, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/andreiliphd/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/andreiliphd/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "| dictionary: 50264 types\n"
     ]
    }
   ],
   "source": [
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        super(TextDataset, self).__init__()\n",
    "        self.text = text\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        encoded = roberta.encode(self.text.iloc[index].Data)\n",
    "        encoded = torch.nn.functional.pad(encoded, pad=(0,400 - encoded.shape[0]), mode='constant', value=2)\n",
    "        return encoded, self.text.iloc[index].Label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_text = torch.utils.data.DataLoader(TextDataset(text),batch_size=64, shuffle=True)\n",
    "dl_text_test = torch.utils.data.DataLoader(TextDataset(text_test),batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoBERTaClassifier, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1024, 128)\n",
    "        self.fc2 = torch.nn.Linear(51200, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 2)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = roberta.extract_features(x)        \n",
    "        x = torch.nn.functional.relu(self.fc1(x))        \n",
    "        x = torch.nn.functional.relu(self.fc2(x.view(-1, 51200)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RoBERTaClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in roberta.parameters():\n",
    "    par.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (decoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerSentenceEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "roberta.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramters = list(model.parameters()) + list(roberta.parameters())\n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6915871500968933\n",
      "1.0390114784240723\n",
      "1.9574806690216064\n",
      "1.7543612718582153\n",
      "0.6845536231994629\n",
      "1.0137183666229248\n",
      "0.8264726400375366\n",
      "0.6764806509017944\n",
      "0.6742739081382751\n",
      "0.7091284990310669\n",
      "0.8201663494110107\n",
      "0.7052427530288696\n",
      "0.7032874822616577\n",
      "0.7058701515197754\n",
      "0.7022926807403564\n",
      "0.6695496439933777\n",
      "0.7233829498291016\n",
      "0.7077049016952515\n",
      "0.7027021646499634\n",
      "0.686415433883667\n",
      "0.6998987793922424\n",
      "0.6848814487457275\n",
      "0.6758337020874023\n",
      "0.6978969573974609\n",
      "0.7021273970603943\n",
      "0.6803317666053772\n",
      "0.687629759311676\n",
      "0.6858237981796265\n",
      "0.6737041473388672\n",
      "0.6928789019584656\n",
      "0.6811394095420837\n",
      "0.7050673961639404\n",
      "0.6682025194168091\n",
      "0.6759660243988037\n",
      "0.6677019000053406\n",
      "0.7132917642593384\n",
      "0.6692762970924377\n",
      "0.6785686612129211\n",
      "0.6971730589866638\n",
      "0.6520213484764099\n",
      "0.6855332255363464\n",
      "0.6697025299072266\n",
      "0.6928859949111938\n",
      "0.7090451121330261\n",
      "0.6908478140830994\n",
      "0.6799947619438171\n",
      "0.6832920908927917\n",
      "0.6722414493560791\n",
      "0.6878982782363892\n",
      "0.6927591562271118\n",
      "Epoch:  0 , Loss:  0.754145964384079\n",
      "Training accuracy: 54%\n",
      "Test accuracy: 52%\n",
      "0.6939345002174377\n",
      "0.7117504477500916\n",
      "0.6688985824584961\n",
      "0.6717131733894348\n",
      "0.681529700756073\n",
      "0.6569919586181641\n",
      "0.6765328049659729\n",
      "0.6537786722183228\n",
      "0.6665582656860352\n",
      "0.7087717056274414\n",
      "0.7212866544723511\n",
      "0.657404899597168\n",
      "0.6616268754005432\n",
      "0.6541919112205505\n",
      "0.6952106952667236\n",
      "0.6411188840866089\n",
      "0.6933135390281677\n",
      "0.6459131240844727\n",
      "0.6136209964752197\n",
      "0.6882447004318237\n",
      "0.7188100218772888\n",
      "0.6663155555725098\n",
      "0.6645016074180603\n",
      "0.6131351590156555\n",
      "0.6655846834182739\n",
      "0.6389862895011902\n",
      "0.6629515290260315\n",
      "0.6662028431892395\n",
      "0.6720186471939087\n",
      "0.6831976175308228\n",
      "0.6716375350952148\n",
      "0.6860236525535583\n",
      "0.6445544958114624\n",
      "0.617968738079071\n",
      "0.7018791437149048\n",
      "0.6841097474098206\n",
      "0.667272686958313\n",
      "0.6716088652610779\n",
      "0.6671510338783264\n",
      "0.6632553935050964\n",
      "0.6476415991783142\n",
      "0.6825213432312012\n",
      "0.6744321584701538\n",
      "0.6358577609062195\n",
      "0.658169686794281\n",
      "0.6550976634025574\n",
      "0.6554323434829712\n",
      "0.6578115224838257\n",
      "0.666562557220459\n",
      "0.6663143038749695\n",
      "Epoch:  1 , Loss:  0.6677879655361175\n",
      "Training accuracy: 59%\n",
      "Test accuracy: 57%\n",
      "0.6789981722831726\n",
      "0.6253715753555298\n",
      "0.6305917501449585\n",
      "0.6371816396713257\n",
      "0.6150768399238586\n",
      "0.6245914697647095\n",
      "0.6713022589683533\n",
      "0.631702721118927\n",
      "0.6355537176132202\n",
      "0.6137567162513733\n",
      "0.6131100058555603\n",
      "0.628020167350769\n",
      "0.5396647453308105\n",
      "0.6891925930976868\n",
      "0.6346609592437744\n",
      "0.6946773529052734\n",
      "0.5898079872131348\n",
      "0.6470624208450317\n",
      "0.5901852250099182\n",
      "0.6119452714920044\n",
      "0.6057910919189453\n",
      "0.5874888300895691\n",
      "0.6214874386787415\n",
      "0.6065961718559265\n",
      "0.6194638013839722\n",
      "0.5931130051612854\n",
      "0.6661900877952576\n",
      "0.604814887046814\n",
      "0.5796167850494385\n",
      "0.6833970546722412\n",
      "0.6013131141662598\n",
      "0.6344111561775208\n",
      "0.5956026315689087\n",
      "0.5921887755393982\n",
      "0.582719624042511\n",
      "0.5564401149749756\n",
      "0.6572182774543762\n",
      "0.678068220615387\n",
      "0.6737706661224365\n",
      "0.5571971535682678\n",
      "0.5446830987930298\n",
      "0.5972772240638733\n",
      "0.602634847164154\n",
      "0.508213460445404\n",
      "0.5398644208908081\n",
      "0.6206108331680298\n",
      "0.5885834693908691\n",
      "0.634465217590332\n",
      "0.6599733829498291\n",
      "0.5004145503044128\n",
      "Epoch:  2 , Loss:  0.6139212596416473\n",
      "Training accuracy: 70%\n",
      "Test accuracy: 67%\n",
      "0.5450421571731567\n",
      "0.5591323971748352\n",
      "0.551598846912384\n",
      "0.5808491110801697\n",
      "0.5857776999473572\n",
      "0.6519313454627991\n",
      "0.5670633912086487\n",
      "0.5987723469734192\n",
      "0.6067642569541931\n",
      "0.5721665620803833\n",
      "0.5203781723976135\n",
      "0.6244840621948242\n",
      "0.5716537833213806\n",
      "0.5760132074356079\n",
      "0.5193782448768616\n",
      "0.5935817360877991\n",
      "0.5382360816001892\n",
      "0.4861074984073639\n",
      "0.6081334948539734\n",
      "0.4888991117477417\n",
      "0.5780433416366577\n",
      "0.5369691848754883\n",
      "0.5911797881126404\n",
      "0.5071172714233398\n",
      "0.514973521232605\n",
      "0.5582146048545837\n",
      "0.595589280128479\n",
      "0.5494417548179626\n",
      "0.6175293326377869\n",
      "0.6099033355712891\n",
      "0.5606085062026978\n",
      "0.550111711025238\n",
      "0.5884503126144409\n",
      "0.603461742401123\n",
      "0.5265724658966064\n",
      "0.5383670926094055\n",
      "0.5825284123420715\n",
      "0.6299269795417786\n",
      "0.5538884401321411\n",
      "0.5088367462158203\n",
      "0.6908949017524719\n",
      "0.5526588559150696\n",
      "0.4735976457595825\n",
      "0.4968789219856262\n",
      "0.5579700469970703\n",
      "0.4488641321659088\n",
      "0.6240044236183167\n",
      "0.558003306388855\n",
      "0.49244970083236694\n",
      "0.5371597409248352\n",
      "Epoch:  3 , Loss:  0.5616031801700592\n",
      "Training accuracy: 70%\n",
      "Test accuracy: 65%\n",
      "0.6559505462646484\n",
      "0.5461809039115906\n",
      "0.5661611557006836\n",
      "0.5151268839836121\n",
      "0.4904497265815735\n",
      "0.4403839409351349\n",
      "0.5519136786460876\n",
      "0.43532994389533997\n",
      "0.48617908358573914\n",
      "0.5243768095970154\n",
      "0.6028283834457397\n",
      "0.4415142834186554\n",
      "0.5313003063201904\n",
      "0.506605327129364\n",
      "0.5329335927963257\n",
      "0.4952709376811981\n",
      "0.5234020352363586\n",
      "0.4361377954483032\n",
      "0.4915733337402344\n",
      "0.4600665271282196\n",
      "0.4693286120891571\n",
      "0.4862484931945801\n",
      "0.4770733416080475\n",
      "0.4264950752258301\n",
      "0.5344057083129883\n",
      "0.46267542243003845\n",
      "0.478374183177948\n",
      "0.4286007881164551\n",
      "0.4896143078804016\n",
      "0.42203694581985474\n",
      "0.4684707224369049\n",
      "0.4972217082977295\n",
      "0.3728136718273163\n",
      "0.5566157102584839\n",
      "0.4600558876991272\n",
      "0.4141770601272583\n",
      "0.46241140365600586\n",
      "0.5917696952819824\n",
      "0.4184058606624603\n",
      "0.5017207264900208\n",
      "0.49568939208984375\n",
      "0.4419955909252167\n",
      "0.43162378668785095\n",
      "0.4670592248439789\n",
      "0.5892387628555298\n",
      "0.5719320178031921\n",
      "0.5356602072715759\n",
      "0.5774862766265869\n",
      "0.507655918598175\n",
      "0.5059810876846313\n",
      "Epoch:  4 , Loss:  0.49553045570850374\n",
      "Training accuracy: 81%\n",
      "Test accuracy: 74%\n",
      "0.47245293855667114\n",
      "0.44790229201316833\n",
      "0.44439712166786194\n",
      "0.37282827496528625\n",
      "0.36859679222106934\n",
      "0.38669613003730774\n",
      "0.4806574583053589\n",
      "0.496179461479187\n",
      "0.4497189521789551\n",
      "0.45076465606689453\n",
      "0.28290989995002747\n",
      "0.3499196469783783\n",
      "0.45887407660484314\n",
      "0.4241783022880554\n",
      "0.4195915460586548\n",
      "0.3920758366584778\n",
      "0.40950125455856323\n",
      "0.4524073004722595\n",
      "0.4724557101726532\n",
      "0.44378891587257385\n",
      "0.3814829885959625\n",
      "0.4080875813961029\n",
      "0.439131498336792\n",
      "0.3770748972892761\n",
      "0.6073720455169678\n",
      "0.3881504535675049\n",
      "0.37987834215164185\n",
      "0.41103094816207886\n",
      "0.4940929114818573\n",
      "0.4473434388637543\n",
      "0.5504150390625\n",
      "0.42334508895874023\n",
      "0.46498432755470276\n",
      "0.37398388981819153\n",
      "0.4121313989162445\n",
      "0.38176411390304565\n",
      "0.4258984327316284\n",
      "0.5335706472396851\n",
      "0.42236196994781494\n",
      "0.3359386920928955\n",
      "0.4944343566894531\n",
      "0.4066897928714752\n",
      "0.4188878536224365\n",
      "0.4786800444126129\n",
      "0.3786560297012329\n",
      "0.4773041009902954\n",
      "0.4586569368839264\n",
      "0.45026543736457825\n",
      "0.43726247549057007\n",
      "0.5232033729553223\n",
      "Epoch:  5 , Loss:  0.4331595134735107\n",
      "Training accuracy: 84%\n",
      "Test accuracy: 73%\n",
      "0.3176247179508209\n",
      "0.43682706356048584\n",
      "0.4330807626247406\n",
      "0.3266172409057617\n",
      "0.37249311804771423\n",
      "0.28935110569000244\n",
      "0.34570151567459106\n",
      "0.3327341079711914\n",
      "0.35417529940605164\n",
      "0.3671388030052185\n",
      "0.33925163745880127\n",
      "0.47489985823631287\n",
      "0.3804801106452942\n",
      "0.3842218518257141\n",
      "0.46032407879829407\n",
      "0.2955491542816162\n",
      "0.372757226228714\n",
      "0.35207441449165344\n",
      "0.3871193826198578\n",
      "0.4426012337207794\n",
      "0.4060591459274292\n",
      "0.3117896318435669\n",
      "0.5737780928611755\n",
      "0.4599267840385437\n",
      "0.3600177764892578\n",
      "0.41846686601638794\n",
      "0.4996936321258545\n",
      "0.3425425589084625\n",
      "0.36065953969955444\n",
      "0.4284173548221588\n",
      "0.44377371668815613\n",
      "0.49106186628341675\n",
      "0.4716701805591583\n",
      "0.5910192131996155\n",
      "0.4359215199947357\n",
      "0.3825390338897705\n",
      "0.4114309251308441\n",
      "0.38210082054138184\n",
      "0.43984320759773254\n",
      "0.44588521122932434\n",
      "0.38394343852996826\n",
      "0.4787963330745697\n",
      "0.5208542346954346\n",
      "0.43713682889938354\n",
      "0.3457118570804596\n",
      "0.4230785369873047\n",
      "0.43189573287963867\n",
      "0.5628015398979187\n",
      "0.41055065393447876\n",
      "0.44160589575767517\n",
      "Epoch:  6 , Loss:  0.41115989625453947\n",
      "Training accuracy: 86%\n",
      "Test accuracy: 70%\n",
      "0.3231937289237976\n",
      "0.29227471351623535\n",
      "0.35555407404899597\n",
      "0.4174327552318573\n",
      "0.3333948254585266\n",
      "0.4467267096042633\n",
      "0.28802165389060974\n",
      "0.3673458397388458\n",
      "0.3332861065864563\n",
      "0.3205956518650055\n",
      "0.37455180287361145\n",
      "0.27905213832855225\n",
      "0.34221717715263367\n",
      "0.3478087782859802\n",
      "0.35911834239959717\n",
      "0.41486749053001404\n",
      "0.35695844888687134\n",
      "0.3248414993286133\n",
      "0.33965203166007996\n",
      "0.3854270279407501\n",
      "0.5207566022872925\n",
      "0.5268375277519226\n",
      "0.4085911512374878\n",
      "0.39034968614578247\n",
      "0.287274032831192\n",
      "0.3535928726196289\n",
      "0.3764609098434448\n",
      "0.30709871649742126\n",
      "0.3195788562297821\n",
      "0.3067396283149719\n",
      "0.3475942611694336\n",
      "0.2981957197189331\n",
      "0.21296460926532745\n",
      "0.35338377952575684\n",
      "0.24818389117717743\n",
      "0.38440844416618347\n",
      "0.3268827795982361\n",
      "0.34584519267082214\n",
      "0.31760120391845703\n",
      "0.3697265684604645\n",
      "0.2815118730068207\n",
      "0.35325923562049866\n",
      "0.2419947236776352\n",
      "0.36223331093788147\n",
      "0.33267173171043396\n",
      "0.2919641137123108\n",
      "0.3886868357658386\n",
      "0.3917827606201172\n",
      "0.32070469856262207\n",
      "0.2833610475063324\n",
      "Epoch:  7 , Loss:  0.34505115121603014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 88%\n",
      "Test accuracy: 74%\n",
      "0.29023799300193787\n",
      "0.2957703173160553\n",
      "0.21815288066864014\n",
      "0.3203911781311035\n",
      "0.4109954833984375\n",
      "0.28615522384643555\n",
      "0.29489612579345703\n",
      "0.2998989224433899\n",
      "0.2777974307537079\n",
      "0.38903212547302246\n",
      "0.33640775084495544\n",
      "0.3273164629936218\n",
      "0.33553558588027954\n",
      "0.29186761379241943\n",
      "0.2555338740348816\n",
      "0.2637486457824707\n",
      "0.3530856966972351\n",
      "0.21011993288993835\n",
      "0.2779747247695923\n",
      "0.29570117592811584\n",
      "0.29375502467155457\n",
      "0.40394216775894165\n",
      "0.27368277311325073\n",
      "0.37538814544677734\n",
      "0.3407171666622162\n",
      "0.30737611651420593\n",
      "0.3050764501094818\n",
      "0.31599724292755127\n",
      "0.33928558230400085\n",
      "0.40997904539108276\n",
      "0.2594504654407501\n",
      "0.2758556008338928\n",
      "0.26744791865348816\n",
      "0.2971087694168091\n",
      "0.19979417324066162\n",
      "0.3262222409248352\n",
      "0.31175580620765686\n",
      "0.3174740970134735\n",
      "0.24521255493164062\n",
      "0.29265451431274414\n",
      "0.3161262571811676\n",
      "0.3555377423763275\n",
      "0.3603319525718689\n",
      "0.34288719296455383\n",
      "0.32132092118263245\n",
      "0.36510229110717773\n",
      "0.3631526231765747\n",
      "0.315531462430954\n",
      "0.3997563123703003\n",
      "0.40395066142082214\n",
      "Epoch:  8 , Loss:  0.3146498483419418\n",
      "Training accuracy: 89%\n",
      "Test accuracy: 74%\n",
      "0.3319582939147949\n",
      "0.25239256024360657\n",
      "0.2989499866962433\n",
      "0.32022523880004883\n",
      "0.2213125079870224\n",
      "0.21469557285308838\n",
      "0.28577175736427307\n",
      "0.32887306809425354\n",
      "0.20457245409488678\n",
      "0.2715945541858673\n",
      "0.3471459746360779\n",
      "0.3207505941390991\n",
      "0.2290007770061493\n",
      "0.2572842538356781\n",
      "0.30025508999824524\n",
      "0.3330385386943817\n",
      "0.3683946132659912\n",
      "0.3023119568824768\n",
      "0.2224394530057907\n",
      "0.2299249768257141\n",
      "0.2940475344657898\n",
      "0.18981516361236572\n",
      "0.29463979601860046\n",
      "0.23408140242099762\n",
      "0.320942759513855\n",
      "0.2533106803894043\n",
      "0.3259161412715912\n",
      "0.3752971291542053\n",
      "0.3333161473274231\n",
      "0.3431446850299835\n",
      "0.3397332727909088\n",
      "0.25403618812561035\n",
      "0.298138827085495\n",
      "0.18757876753807068\n",
      "0.4781301021575928\n",
      "0.33992043137550354\n",
      "0.3204987943172455\n",
      "0.30815941095352173\n",
      "0.34572720527648926\n",
      "0.335673063993454\n",
      "0.24213747680187225\n",
      "0.22027403116226196\n",
      "0.338356614112854\n",
      "0.4271547198295593\n",
      "0.36176347732543945\n",
      "0.25794997811317444\n",
      "0.1937289983034134\n",
      "0.3571040630340576\n",
      "0.2551541030406952\n",
      "0.1990271955728531\n",
      "Epoch:  9 , Loss:  0.2933130076527595\n",
      "Training accuracy: 90%\n",
      "Test accuracy: 74%\n",
      "0.20080207288265228\n",
      "0.24163484573364258\n",
      "0.25325995683670044\n",
      "0.19636270403862\n",
      "0.3137739598751068\n",
      "0.21236063539981842\n",
      "0.2747373878955841\n",
      "0.24702608585357666\n",
      "0.27087852358818054\n",
      "0.2694709002971649\n",
      "0.2018664926290512\n",
      "0.40428805351257324\n",
      "0.305148720741272\n",
      "0.3075057864189148\n",
      "0.2331230640411377\n",
      "0.2534245252609253\n",
      "0.2586112320423126\n",
      "0.31200164556503296\n",
      "0.2610754370689392\n",
      "0.2570043206214905\n",
      "0.23796045780181885\n",
      "0.25728559494018555\n",
      "0.322291761636734\n",
      "0.24488961696624756\n",
      "0.2090500444173813\n",
      "0.37800133228302\n",
      "0.2865023910999298\n",
      "0.19161850214004517\n",
      "0.2395525872707367\n",
      "0.2871244549751282\n",
      "0.21755759418010712\n",
      "0.23751860857009888\n",
      "0.21511989831924438\n",
      "0.281603068113327\n",
      "0.33298686146736145\n",
      "0.22667983174324036\n",
      "0.24519138038158417\n",
      "0.22675111889839172\n",
      "0.20095819234848022\n",
      "0.2632332444190979\n",
      "0.17765268683433533\n",
      "0.2568504214286804\n",
      "0.22777172923088074\n",
      "0.2052430659532547\n",
      "0.2272072732448578\n",
      "0.2182590365409851\n",
      "0.3389456868171692\n",
      "0.21001528203487396\n",
      "0.19173017144203186\n",
      "0.34985464811325073\n",
      "Epoch:  10 , Loss:  0.2555952578783035\n",
      "Training accuracy: 91%\n",
      "Test accuracy: 73%\n",
      "0.22457076609134674\n",
      "0.36689046025276184\n",
      "0.2904234528541565\n",
      "0.27769702672958374\n",
      "0.2036859542131424\n",
      "0.25106191635131836\n",
      "0.27509555220603943\n",
      "0.31406110525131226\n",
      "0.16317832469940186\n",
      "0.22826062142848969\n",
      "0.19820092618465424\n",
      "0.24770377576351166\n",
      "0.26856639981269836\n",
      "0.17396113276481628\n",
      "0.22272400557994843\n",
      "0.23318326473236084\n",
      "0.17934265732765198\n",
      "0.18387018144130707\n",
      "0.22334769368171692\n",
      "0.25333404541015625\n",
      "0.2894600033760071\n",
      "0.27214184403419495\n",
      "0.35779568552970886\n",
      "0.19341029226779938\n",
      "0.19611844420433044\n",
      "0.2084794044494629\n",
      "0.2537354528903961\n",
      "0.19193226099014282\n",
      "0.22380195558071136\n",
      "0.3162555694580078\n",
      "0.26152899861335754\n",
      "0.278834730386734\n",
      "0.25683143734931946\n",
      "0.24854546785354614\n",
      "0.20175670087337494\n",
      "0.2752838134765625\n",
      "0.31569480895996094\n",
      "0.18376785516738892\n",
      "0.3222643733024597\n",
      "0.2944473922252655\n",
      "0.30043643712997437\n",
      "0.32394251227378845\n",
      "0.2682567834854126\n",
      "0.2626168131828308\n",
      "0.21198770403862\n",
      "0.19078275561332703\n",
      "0.2682494521141052\n",
      "0.21079519391059875\n",
      "0.20891577005386353\n",
      "0.19142916798591614\n",
      "Epoch:  11 , Loss:  0.24717316687107085\n",
      "Training accuracy: 92%\n",
      "Test accuracy: 75%\n",
      "0.18330912292003632\n",
      "0.2209809124469757\n",
      "0.15178820490837097\n",
      "0.192857563495636\n",
      "0.2945041060447693\n",
      "0.21805375814437866\n",
      "0.26610782742500305\n",
      "0.17223674058914185\n",
      "0.22001546621322632\n",
      "0.195807084441185\n",
      "0.19136393070220947\n",
      "0.14879998564720154\n",
      "0.23588262498378754\n",
      "0.2235070914030075\n",
      "0.1547318547964096\n",
      "0.1853085309267044\n",
      "0.25262731313705444\n",
      "0.21890120208263397\n",
      "0.2519494891166687\n",
      "0.1897379457950592\n",
      "0.2837849259376526\n",
      "0.19116899371147156\n",
      "0.17654454708099365\n",
      "0.24002711474895477\n",
      "0.21871069073677063\n",
      "0.23260939121246338\n",
      "0.17689627408981323\n",
      "0.2256493866443634\n",
      "0.19103464484214783\n",
      "0.19101114571094513\n",
      "0.3241420090198517\n",
      "0.14865107834339142\n",
      "0.22370539605617523\n",
      "0.34072500467300415\n",
      "0.1992754489183426\n",
      "0.23646169900894165\n",
      "0.403253436088562\n",
      "0.27340689301490784\n",
      "0.2437506914138794\n",
      "0.21831266582012177\n",
      "0.26691800355911255\n",
      "0.19115552306175232\n",
      "0.25021839141845703\n",
      "0.21237502992153168\n",
      "0.20900626480579376\n",
      "0.22676528990268707\n",
      "0.24494852125644684\n",
      "0.23296163976192474\n",
      "0.24137181043624878\n",
      "0.323688268661499\n",
      "Epoch:  12 , Loss:  0.22614001870155334\n",
      "Training accuracy: 94%\n",
      "Test accuracy: 75%\n",
      "0.1301746368408203\n",
      "0.16371838748455048\n",
      "0.1991279572248459\n",
      "0.22278040647506714\n",
      "0.13847306370735168\n",
      "0.17882704734802246\n",
      "0.17874139547348022\n",
      "0.08921714872121811\n",
      "0.23792040348052979\n",
      "0.13378643989562988\n",
      "0.17150543630123138\n",
      "0.31080523133277893\n",
      "0.24487777054309845\n",
      "0.1591748595237732\n",
      "0.2180723398923874\n",
      "0.2896793484687805\n",
      "0.22415997087955475\n",
      "0.16318179666996002\n",
      "0.1734568178653717\n",
      "0.1713578999042511\n",
      "0.2580908238887787\n",
      "0.10641525685787201\n",
      "0.18632175028324127\n",
      "0.1811019778251648\n",
      "0.2572423815727234\n",
      "0.33612245321273804\n",
      "0.1410125344991684\n",
      "0.37845727801322937\n",
      "0.25979650020599365\n",
      "0.23030169308185577\n",
      "0.23578907549381256\n",
      "0.13480034470558167\n",
      "0.23633772134780884\n",
      "0.16909798979759216\n",
      "0.15581545233726501\n",
      "0.2531776428222656\n",
      "0.2183544635772705\n",
      "0.1451074779033661\n",
      "0.18981941044330597\n",
      "0.21121686697006226\n",
      "0.16985249519348145\n",
      "0.2954205870628357\n",
      "0.32795989513397217\n",
      "0.13114844262599945\n",
      "0.3101932108402252\n",
      "0.13680677115917206\n",
      "0.2823801040649414\n",
      "0.17991429567337036\n",
      "0.20006483793258667\n",
      "0.2027938812971115\n",
      "Epoch:  13 , Loss:  0.2063990394771099\n",
      "Training accuracy: 93%\n",
      "Test accuracy: 75%\n",
      "0.2398863136768341\n",
      "0.18426087498664856\n",
      "0.25040382146835327\n",
      "0.1488083153963089\n",
      "0.15113283693790436\n",
      "0.197616845369339\n",
      "0.21756502985954285\n",
      "0.15039169788360596\n",
      "0.19992928206920624\n",
      "0.34166857600212097\n",
      "0.30837833881378174\n",
      "0.19973890483379364\n",
      "0.22666466236114502\n",
      "0.1881762593984604\n",
      "0.21540510654449463\n",
      "0.15580439567565918\n",
      "0.18305762112140656\n",
      "0.3181452751159668\n",
      "0.2018144130706787\n",
      "0.2382616102695465\n",
      "0.18984395265579224\n",
      "0.1877019703388214\n",
      "0.1567145735025406\n",
      "0.16066980361938477\n",
      "0.17649877071380615\n",
      "0.2578740119934082\n",
      "0.190914124250412\n",
      "0.20317885279655457\n",
      "0.15268976986408234\n",
      "0.18364043533802032\n",
      "0.15672744810581207\n",
      "0.16829976439476013\n",
      "0.24086007475852966\n",
      "0.22579926252365112\n",
      "0.17100177705287933\n",
      "0.17090724408626556\n",
      "0.20546506345272064\n",
      "0.24396969377994537\n",
      "0.20286722481250763\n",
      "0.253882497549057\n",
      "0.14311809837818146\n",
      "0.22078092396259308\n",
      "0.20595155656337738\n",
      "0.19377604126930237\n",
      "0.2054443657398224\n",
      "0.2926359474658966\n",
      "0.12212909758090973\n",
      "0.21646106243133545\n",
      "0.1519274115562439\n",
      "0.2764725387096405\n",
      "Epoch:  14 , Loss:  0.20490627080202103\n",
      "Training accuracy: 95%\n",
      "Test accuracy: 75%\n",
      "0.15685056149959564\n",
      "0.1233847588300705\n",
      "0.18383583426475525\n",
      "0.24995288252830505\n",
      "0.16376863420009613\n",
      "0.1156158596277237\n",
      "0.13634566962718964\n",
      "0.19861403107643127\n",
      "0.25577977299690247\n",
      "0.1792246550321579\n",
      "0.12802457809448242\n",
      "0.25003233551979065\n",
      "0.18089455366134644\n",
      "0.22748228907585144\n",
      "0.1821366548538208\n",
      "0.21110451221466064\n",
      "0.10084007680416107\n",
      "0.1965004950761795\n",
      "0.23359167575836182\n",
      "0.28635111451148987\n",
      "0.26197999715805054\n",
      "0.258674293756485\n",
      "0.2670682668685913\n",
      "0.11707723885774612\n",
      "0.15676356852054596\n",
      "0.19232229888439178\n",
      "0.12382373213768005\n",
      "0.12700022757053375\n",
      "0.15690724551677704\n",
      "0.19223669171333313\n",
      "0.13054746389389038\n",
      "0.1874454766511917\n",
      "0.13691166043281555\n",
      "0.1485302448272705\n",
      "0.18353642523288727\n",
      "0.17953626811504364\n",
      "0.2543947100639343\n",
      "0.22591231763362885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21455368399620056\n",
      "0.2740834355354309\n",
      "0.28194114565849304\n",
      "0.17101649940013885\n",
      "0.23863202333450317\n",
      "0.20043639838695526\n",
      "0.2323230504989624\n",
      "0.1492769867181778\n",
      "0.07663100957870483\n",
      "0.15478159487247467\n",
      "0.19222235679626465\n",
      "0.12686000764369965\n",
      "Epoch:  15 , Loss:  0.1874751453101635\n",
      "Training accuracy: 95%\n",
      "Test accuracy: 74%\n",
      "0.21557632088661194\n",
      "0.20380406081676483\n",
      "0.20362624526023865\n",
      "0.11167893558740616\n",
      "0.17940926551818848\n",
      "0.13232775032520294\n",
      "0.18194934725761414\n",
      "0.12561379373073578\n",
      "0.10239872336387634\n",
      "0.08991820365190506\n",
      "0.15175320208072662\n",
      "0.1260189712047577\n",
      "0.1284562051296234\n",
      "0.12321250140666962\n",
      "0.13289684057235718\n",
      "0.131461039185524\n",
      "0.13023824989795685\n",
      "0.1354255974292755\n",
      "0.2050091177225113\n",
      "0.13111086189746857\n",
      "0.19629232585430145\n",
      "0.16270489990711212\n",
      "0.1934753954410553\n",
      "0.14222319424152374\n",
      "0.11146748065948486\n",
      "0.11498935520648956\n",
      "0.10064157098531723\n",
      "0.18288250267505646\n",
      "0.2002207189798355\n",
      "0.14114636182785034\n",
      "0.251921147108078\n",
      "0.12674978375434875\n",
      "0.18579521775245667\n",
      "0.19575752317905426\n",
      "0.1141858845949173\n",
      "0.1690371185541153\n",
      "0.17165490984916687\n",
      "0.13045911490917206\n",
      "0.11300063133239746\n",
      "0.1327163428068161\n",
      "0.09326294809579849\n",
      "0.12627944350242615\n",
      "0.18294362723827362\n",
      "0.24539080262184143\n",
      "0.23129145801067352\n",
      "0.12814861536026\n",
      "0.2624662220478058\n",
      "0.14604705572128296\n",
      "0.2650248408317566\n",
      "0.16710636019706726\n",
      "Epoch:  16 , Loss:  0.158543361723423\n",
      "Training accuracy: 95%\n",
      "Test accuracy: 74%\n",
      "0.1886489987373352\n",
      "0.09909213334321976\n",
      "0.15669743716716766\n",
      "0.09922406077384949\n",
      "0.1546967625617981\n",
      "0.2656444311141968\n",
      "0.09444776922464371\n",
      "0.17426814138889313\n",
      "0.13864582777023315\n",
      "0.21410132944583893\n",
      "0.13109757006168365\n",
      "0.09701306372880936\n",
      "0.32118135690689087\n",
      "0.27072393894195557\n",
      "0.1624322533607483\n",
      "0.16265156865119934\n",
      "0.12341601401567459\n",
      "0.13650833070278168\n",
      "0.22087231278419495\n",
      "0.11840476095676422\n",
      "0.13921037316322327\n",
      "0.16023331880569458\n",
      "0.30325254797935486\n",
      "0.11708962917327881\n",
      "0.09041421860456467\n",
      "0.12092572450637817\n",
      "0.21184217929840088\n",
      "0.10908323526382446\n",
      "0.12443606555461884\n",
      "0.2501072883605957\n",
      "0.16445600986480713\n",
      "0.11685705184936523\n",
      "0.1213768869638443\n",
      "0.21604937314987183\n",
      "0.1706215739250183\n",
      "0.15522558987140656\n",
      "0.07880489528179169\n",
      "0.17548511922359467\n",
      "0.1375398337841034\n",
      "0.19617347419261932\n",
      "0.19451400637626648\n",
      "0.1101071834564209\n",
      "0.15560276806354523\n",
      "0.09007715433835983\n",
      "0.1467691957950592\n",
      "0.13271495699882507\n",
      "0.13028809428215027\n",
      "0.12508803606033325\n",
      "0.08779320120811462\n",
      "0.2281961888074875\n",
      "Epoch:  17 , Loss:  0.15780206471681596\n",
      "Training accuracy: 94%\n",
      "Test accuracy: 72%\n",
      "0.17886745929718018\n",
      "0.10849469900131226\n",
      "0.22761310636997223\n",
      "0.07972518354654312\n",
      "0.09788110107183456\n",
      "0.1127275675535202\n",
      "0.09314538538455963\n",
      "0.11102381348609924\n",
      "0.08703991025686264\n",
      "0.16397348046302795\n",
      "0.11897128820419312\n",
      "0.11788609623908997\n",
      "0.07342655211687088\n",
      "0.18257907032966614\n",
      "0.12628619372844696\n",
      "0.10908807814121246\n",
      "0.07523298263549805\n",
      "0.1727556437253952\n",
      "0.09518085420131683\n",
      "0.12092329561710358\n",
      "0.13829389214515686\n",
      "0.07399416714906693\n",
      "0.09922376275062561\n",
      "0.20110535621643066\n",
      "0.21059727668762207\n",
      "0.12412163615226746\n",
      "0.131686732172966\n",
      "0.10375457257032394\n",
      "0.2026473432779312\n",
      "0.14926868677139282\n",
      "0.14421826601028442\n",
      "0.0880281925201416\n",
      "0.08304253220558167\n",
      "0.12121221423149109\n",
      "0.09969191998243332\n",
      "0.09849175065755844\n",
      "0.1457173079252243\n",
      "0.15056827664375305\n",
      "0.15908853709697723\n",
      "0.27718719840049744\n",
      "0.21407438814640045\n",
      "0.16535742580890656\n",
      "0.17301106452941895\n",
      "0.21850724518299103\n",
      "0.11027006059885025\n",
      "0.1711660772562027\n",
      "0.13860824704170227\n",
      "0.11698608845472336\n",
      "0.12987108528614044\n",
      "0.15398523211479187\n",
      "Epoch:  18 , Loss:  0.13693196594715118\n",
      "Training accuracy: 95%\n",
      "Test accuracy: 73%\n",
      "0.18542958796024323\n",
      "0.08606116473674774\n",
      "0.07753177732229233\n",
      "0.060038257390260696\n",
      "0.07398973405361176\n",
      "0.059383925050497055\n",
      "0.1507403552532196\n",
      "0.12923604249954224\n",
      "0.10575176775455475\n",
      "0.1421092450618744\n",
      "0.06459304690361023\n",
      "0.12058339267969131\n",
      "0.0843423455953598\n",
      "0.14834041893482208\n",
      "0.17163020372390747\n",
      "0.15390224754810333\n",
      "0.06599250435829163\n",
      "0.16552868485450745\n",
      "0.16356906294822693\n",
      "0.12244845926761627\n",
      "0.11326015740633011\n",
      "0.12417424470186234\n",
      "0.2360900640487671\n",
      "0.18916529417037964\n",
      "0.12015276402235031\n",
      "0.11294959485530853\n",
      "0.22773092985153198\n",
      "0.09310095757246017\n",
      "0.07887838780879974\n",
      "0.11198092252016068\n",
      "0.1936943382024765\n",
      "0.17410655319690704\n",
      "0.18394289910793304\n",
      "0.17962999641895294\n",
      "0.301690012216568\n",
      "0.2731318175792694\n",
      "0.19004876911640167\n",
      "0.07016774266958237\n",
      "0.33740612864494324\n",
      "0.1753252148628235\n",
      "0.1321236789226532\n",
      "0.0965779572725296\n",
      "0.12435317039489746\n",
      "0.19014857709407806\n",
      "0.1382521688938141\n",
      "0.15872405469417572\n",
      "0.1055702343583107\n",
      "0.13341635465621948\n",
      "0.17676939070224762\n",
      "0.1278408020734787\n",
      "Epoch:  19 , Loss:  0.14403210803866387\n",
      "Training accuracy: 96%\n",
      "Test accuracy: 75%\n",
      "0.1104244664311409\n",
      "0.11086004227399826\n",
      "0.137549489736557\n",
      "0.12613427639007568\n",
      "0.15406695008277893\n",
      "0.07335102558135986\n",
      "0.06774280220270157\n",
      "0.08590742945671082\n",
      "0.11551754176616669\n",
      "0.17756636440753937\n",
      "0.08185531198978424\n",
      "0.07912163436412811\n",
      "0.11412492394447327\n",
      "0.11267727613449097\n",
      "0.14919674396514893\n",
      "0.09582637250423431\n",
      "0.09373156726360321\n",
      "0.12261820584535599\n",
      "0.1649661809206009\n",
      "0.08635333925485611\n",
      "0.08133357763290405\n",
      "0.12341004610061646\n",
      "0.1342339813709259\n",
      "0.1422523558139801\n",
      "0.0975218415260315\n",
      "0.0937233418226242\n",
      "0.16123142838478088\n",
      "0.12637212872505188\n",
      "0.13199849426746368\n",
      "0.10295462608337402\n",
      "0.07598792016506195\n",
      "0.1475176215171814\n",
      "0.10962224006652832\n",
      "0.20362354815006256\n",
      "0.14539401233196259\n",
      "0.22915124893188477\n",
      "0.178914874792099\n",
      "0.13325124979019165\n",
      "0.15530553460121155\n",
      "0.13325725495815277\n",
      "0.1509922742843628\n",
      "0.22172744572162628\n",
      "0.1091560423374176\n",
      "0.11361706256866455\n",
      "0.20354370772838593\n",
      "0.0821177214384079\n",
      "0.12492777407169342\n",
      "0.16813558340072632\n",
      "0.0817616879940033\n",
      "0.11517873406410217\n",
      "Epoch:  20 , Loss:  0.1267561461031437\n",
      "Training accuracy: 97%\n",
      "Test accuracy: 73%\n",
      "0.10120239853858948\n",
      "0.07370559126138687\n",
      "0.14375373721122742\n",
      "0.1377604752779007\n",
      "0.09145475924015045\n",
      "0.09057201445102692\n",
      "0.04691915959119797\n",
      "0.06633366644382477\n",
      "0.05231447517871857\n",
      "0.08181516826152802\n",
      "0.1023150235414505\n",
      "0.10386501252651215\n",
      "0.17106664180755615\n",
      "0.09590792655944824\n",
      "0.1386432647705078\n",
      "0.14111919701099396\n",
      "0.05724819004535675\n",
      "0.07841754704713821\n",
      "0.10158495604991913\n",
      "0.16877883672714233\n",
      "0.2042139619588852\n",
      "0.09573468565940857\n",
      "0.13524127006530762\n",
      "0.07213890552520752\n",
      "0.10529009997844696\n",
      "0.1087229996919632\n",
      "0.04660036042332649\n",
      "0.14538918435573578\n",
      "0.12079570442438126\n",
      "0.15190589427947998\n",
      "0.0631307065486908\n",
      "0.13089539110660553\n",
      "0.10487931221723557\n",
      "0.1510036140680313\n",
      "0.1025327742099762\n",
      "0.24216963350772858\n",
      "0.22389277815818787\n",
      "0.17764928936958313\n",
      "0.16523343324661255\n",
      "0.11937105655670166\n",
      "0.1977708339691162\n",
      "0.07900263369083405\n",
      "0.07846124470233917\n",
      "0.26348093152046204\n",
      "0.08842308074235916\n",
      "0.1329934149980545\n",
      "0.1856449693441391\n",
      "0.09807053208351135\n",
      "0.10575714707374573\n",
      "0.14466756582260132\n",
      "Epoch:  21 , Loss:  0.1217168290168047\n",
      "Training accuracy: 96%\n",
      "Test accuracy: 75%\n",
      "0.06955046206712723\n",
      "0.11032255738973618\n",
      "0.062016621232032776\n",
      "0.12453646212816238\n",
      "0.09540475159883499\n",
      "0.11117737740278244\n",
      "0.0912070944905281\n",
      "0.11340656131505966\n",
      "0.0720355212688446\n",
      "0.061970897018909454\n",
      "0.07998349517583847\n",
      "0.08201967179775238\n",
      "0.11632205545902252\n",
      "0.10133902728557587\n",
      "0.14050668478012085\n",
      "0.08021455258131027\n",
      "0.192769855260849\n",
      "0.11241830140352249\n",
      "0.16058151423931122\n",
      "0.05944200977683067\n",
      "0.037397295236587524\n",
      "0.21595017611980438\n",
      "0.11139550805091858\n",
      "0.13765911757946014\n",
      "0.07019902020692825\n",
      "0.06374066323041916\n",
      "0.11247586458921432\n",
      "0.11806720495223999\n",
      "0.08468633145093918\n",
      "0.10559950768947601\n",
      "0.1495213806629181\n",
      "0.10363129526376724\n",
      "0.1287587583065033\n",
      "0.12171955406665802\n",
      "0.1083514466881752\n",
      "0.07205040752887726\n",
      "0.08197434991598129\n",
      "0.0655120462179184\n",
      "0.08057344704866409\n",
      "0.07330455631017685\n",
      "0.15024510025978088\n",
      "0.14767198264598846\n",
      "0.16015389561653137\n",
      "0.10079293698072433\n",
      "0.1456807255744934\n",
      "0.15683668851852417\n",
      "0.10204996168613434\n",
      "0.12486384063959122\n",
      "0.08989334851503372\n",
      "0.060374774038791656\n",
      "Epoch:  22 , Loss:  0.10636713318526744\n",
      "Training accuracy: 97%\n",
      "Test accuracy: 76%\n",
      "0.064906045794487\n",
      "0.04786062613129616\n",
      "0.0993855893611908\n",
      "0.11399352550506592\n",
      "0.15326593816280365\n",
      "0.06970508396625519\n",
      "0.08159064501523972\n",
      "0.09845785796642303\n",
      "0.052775703370571136\n",
      "0.09506919234991074\n",
      "0.09598902612924576\n",
      "0.09939007461071014\n",
      "0.07246863842010498\n",
      "0.09631496667861938\n",
      "0.11580483615398407\n",
      "0.04935823753476143\n",
      "0.0873284861445427\n",
      "0.10567021369934082\n",
      "0.07299618422985077\n",
      "0.10408677160739899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08347001671791077\n",
      "0.13705381751060486\n",
      "0.2068844586610794\n",
      "0.08554056286811829\n",
      "0.10120834410190582\n",
      "0.06842412799596786\n",
      "0.06424069404602051\n",
      "0.04868005961179733\n",
      "0.08326072245836258\n",
      "0.13018059730529785\n",
      "0.1397329717874527\n",
      "0.08071091771125793\n",
      "0.1957976520061493\n",
      "0.08654362708330154\n",
      "0.0795007273554802\n",
      "0.12649798393249512\n",
      "0.054439444094896317\n",
      "0.08764077723026276\n",
      "0.11966076493263245\n",
      "0.1635100394487381\n",
      "0.058672185987234116\n",
      "0.07194890081882477\n",
      "0.17100004851818085\n",
      "0.08032383024692535\n",
      "0.0699554830789566\n",
      "0.07304056733846664\n",
      "0.10675084590911865\n",
      "0.10143287479877472\n",
      "0.0721098780632019\n",
      "0.07353022694587708\n",
      "Epoch:  23 , Loss:  0.0959632158279419\n",
      "Training accuracy: 97%\n",
      "Test accuracy: 74%\n",
      "0.10720987617969513\n",
      "0.1118527501821518\n",
      "0.07294748723506927\n",
      "0.07370191812515259\n",
      "0.037194833159446716\n",
      "0.06217729300260544\n",
      "0.11264973878860474\n",
      "0.025573663413524628\n",
      "0.11737868189811707\n",
      "0.07166926562786102\n",
      "0.028696853667497635\n",
      "0.052742380648851395\n",
      "0.1561078429222107\n",
      "0.1027589961886406\n",
      "0.1369750201702118\n",
      "0.08176209032535553\n",
      "0.12649847567081451\n",
      "0.08059918135404587\n",
      "0.030934417620301247\n",
      "0.08983054757118225\n",
      "0.0874902606010437\n",
      "0.09468837082386017\n",
      "0.11571994423866272\n",
      "0.10025687515735626\n",
      "0.05504855141043663\n",
      "0.12410444021224976\n",
      "0.04673479497432709\n",
      "0.10623431950807571\n",
      "0.14525577425956726\n",
      "0.04135265573859215\n",
      "0.0866253525018692\n",
      "0.0771852508187294\n",
      "0.11164628714323044\n",
      "0.06850068271160126\n",
      "0.09833914041519165\n",
      "0.11054135859012604\n",
      "0.07387224584817886\n",
      "0.08275310695171356\n",
      "0.11357659101486206\n",
      "0.1581304371356964\n",
      "0.21259306371212006\n",
      "0.09370584785938263\n",
      "0.14007042348384857\n",
      "0.07509180903434753\n",
      "0.07492276281118393\n",
      "0.0886836051940918\n",
      "0.05453559011220932\n",
      "0.07335839420557022\n",
      "0.07722190767526627\n",
      "0.11700119078159332\n",
      "Epoch:  24 , Loss:  0.09169004697352648\n",
      "Training accuracy: 97%\n",
      "Test accuracy: 74%\n",
      "0.07625280320644379\n",
      "0.08088990300893784\n",
      "0.07322834432125092\n",
      "0.14714738726615906\n",
      "0.07146286964416504\n",
      "0.08498240262269974\n",
      "0.08056637644767761\n",
      "0.11403512954711914\n",
      "0.1705390363931656\n",
      "0.1065114364027977\n",
      "0.06593190878629684\n",
      "0.1739456206560135\n",
      "0.085299551486969\n",
      "0.05547497048974037\n",
      "0.13927534222602844\n",
      "0.060115378350019455\n",
      "0.05536103621125221\n",
      "0.06571511924266815\n",
      "0.053544819355010986\n",
      "0.04894936829805374\n",
      "0.14880940318107605\n",
      "0.0907987505197525\n",
      "0.031780194491147995\n",
      "0.07267788797616959\n",
      "0.037990860641002655\n",
      "0.11282692104578018\n",
      "0.11494555324316025\n",
      "0.07224008440971375\n",
      "0.09990859031677246\n",
      "0.07959591597318649\n",
      "0.08515678346157074\n",
      "0.19648729264736176\n",
      "0.1415419578552246\n",
      "0.10461112856864929\n",
      "0.19549278914928436\n",
      "0.08846651762723923\n",
      "0.10693995654582977\n",
      "0.10798321664333344\n",
      "0.07041364163160324\n",
      "0.15394656360149384\n",
      "0.05862390249967575\n",
      "0.1373661756515503\n",
      "0.061989378184080124\n",
      "0.07184440642595291\n",
      "0.16514059901237488\n",
      "0.06338542699813843\n",
      "0.24644693732261658\n",
      "0.1265248954296112\n",
      "0.04877486079931259\n",
      "0.2291003167629242\n",
      "Epoch:  25 , Loss:  0.10262079425156116\n",
      "Training accuracy: 96%\n",
      "Test accuracy: 72%\n",
      "0.11345276981592178\n",
      "0.11134190112352371\n",
      "0.0742829442024231\n",
      "0.08459910750389099\n",
      "0.08672602474689484\n",
      "0.05597561225295067\n",
      "0.08692875504493713\n",
      "0.07616081088781357\n",
      "0.08686156570911407\n",
      "0.03483056277036667\n",
      "0.14055612683296204\n",
      "0.049006059765815735\n",
      "0.08355286717414856\n",
      "0.07045777142047882\n",
      "0.02869396284222603\n",
      "0.0891745314002037\n",
      "0.17981259524822235\n",
      "0.08084526658058167\n",
      "0.0690869688987732\n",
      "0.08337683230638504\n",
      "0.09943946450948715\n",
      "0.09520172327756882\n",
      "0.08713403344154358\n",
      "0.06229083612561226\n",
      "0.06666866689920425\n",
      "0.11099248379468918\n",
      "0.14397485554218292\n",
      "0.09709218889474869\n",
      "0.048837654292583466\n",
      "0.07412204146385193\n",
      "0.10385319590568542\n",
      "0.1573869287967682\n",
      "0.10179280489683151\n",
      "0.12389621883630753\n",
      "0.08953496813774109\n",
      "0.11385133862495422\n",
      "0.05027713626623154\n",
      "0.06743939965963364\n",
      "0.05142011493444443\n",
      "0.08810682594776154\n",
      "0.04575251415371895\n",
      "0.07952699065208435\n",
      "0.07658575475215912\n",
      "0.05804268270730972\n",
      "0.13313770294189453\n",
      "0.03054070845246315\n",
      "0.07372809946537018\n",
      "0.08011867105960846\n",
      "0.07210341095924377\n",
      "0.046070441603660583\n",
      "Epoch:  26 , Loss:  0.08429285787045955\n",
      "Training accuracy: 97%\n",
      "Test accuracy: 73%\n",
      "0.07651117444038391\n",
      "0.059822697192430496\n",
      "0.11163444817066193\n",
      "0.06887104362249374\n",
      "0.10571323335170746\n",
      "0.11076632142066956\n",
      "0.027249131351709366\n",
      "0.047156937420368195\n",
      "0.04624936729669571\n",
      "0.08688268065452576\n",
      "0.07872676849365234\n",
      "0.05430501699447632\n",
      "0.16168946027755737\n",
      "0.11200407892465591\n",
      "0.1367006003856659\n",
      "0.09383507072925568\n",
      "0.07227659970521927\n",
      "0.05153333395719528\n",
      "0.1581716537475586\n",
      "0.03909062221646309\n",
      "0.08574111014604568\n",
      "0.07547087222337723\n",
      "0.04768325388431549\n",
      "0.05610235780477524\n",
      "0.08305095136165619\n",
      "0.0938834697008133\n",
      "0.04728129133582115\n",
      "0.059950802475214005\n",
      "0.08765798807144165\n",
      "0.04105031117796898\n",
      "0.05599798634648323\n",
      "0.0701056718826294\n",
      "0.11173823475837708\n",
      "0.03713907673954964\n",
      "0.060563527047634125\n",
      "0.06208784133195877\n",
      "0.14918223023414612\n",
      "0.10573962330818176\n",
      "0.06378424167633057\n",
      "0.14873076975345612\n",
      "0.1798660308122635\n",
      "0.2555023431777954\n",
      "0.05686352774500847\n",
      "0.18501174449920654\n",
      "0.10068468004465103\n",
      "0.188196063041687\n",
      "0.052049364894628525\n",
      "0.07585640996694565\n",
      "0.11004593968391418\n",
      "0.11713255941867828\n",
      "Epoch:  27 , Loss:  0.091266810297966\n",
      "Training accuracy: 97%\n",
      "Test accuracy: 73%\n",
      "0.12751558423042297\n",
      "0.10358597338199615\n",
      "0.11130990087985992\n",
      "0.04766452684998512\n",
      "0.13408102095127106\n",
      "0.04389798641204834\n",
      "0.09261973202228546\n",
      "0.09811379015445709\n",
      "0.11214061826467514\n",
      "0.05978325009346008\n",
      "0.05853302776813507\n",
      "0.08178919553756714\n",
      "0.09623756259679794\n",
      "0.032818522304296494\n",
      "0.13406506180763245\n",
      "0.04715246707201004\n",
      "0.13892187178134918\n",
      "0.1203547939658165\n",
      "0.07150726020336151\n",
      "0.038245540112257004\n",
      "0.08968380838632584\n",
      "0.1438155174255371\n",
      "0.07967403531074524\n",
      "0.07330988347530365\n",
      "0.06554631888866425\n",
      "0.07038465887308121\n",
      "0.12804685533046722\n",
      "0.0795307531952858\n",
      "0.022810231894254684\n",
      "0.03842790052294731\n",
      "0.14708170294761658\n",
      "0.1279798448085785\n",
      "0.09315095096826553\n",
      "0.12458173930644989\n",
      "0.06159570440649986\n",
      "0.07934637367725372\n",
      "0.04367472976446152\n",
      "0.06436198949813843\n",
      "0.09697844088077545\n",
      "0.0657491683959961\n",
      "0.06233454495668411\n",
      "0.1540515422821045\n",
      "0.04953416436910629\n",
      "0.12260645627975464\n",
      "0.06977909803390503\n",
      "0.079225555062294\n",
      "0.11314448714256287\n",
      "0.06401769816875458\n",
      "0.08152979612350464\n",
      "0.09949250519275665\n",
      "Epoch:  28 , Loss:  0.0868356828391552\n",
      "Training accuracy: 98%\n",
      "Test accuracy: 74%\n",
      "0.036974553018808365\n",
      "0.07300146669149399\n",
      "0.11945322901010513\n",
      "0.05716542527079582\n",
      "0.08477213233709335\n",
      "0.047590211033821106\n",
      "0.08639238029718399\n",
      "0.03582112118601799\n",
      "0.052208758890628815\n",
      "0.18026581406593323\n",
      "0.0799378901720047\n",
      "0.054978147149086\n",
      "0.07413432002067566\n",
      "0.12723687291145325\n",
      "0.03719782456755638\n",
      "0.09049790352582932\n",
      "0.1896224021911621\n",
      "0.04266125708818436\n",
      "0.09781314432621002\n",
      "0.09928062558174133\n",
      "0.0778321772813797\n",
      "0.10658363997936249\n",
      "0.04145805165171623\n",
      "0.06610056012868881\n",
      "0.17762722074985504\n",
      "0.18130610883235931\n",
      "0.16118989884853363\n",
      "0.034935660660266876\n",
      "0.07273255288600922\n",
      "0.10166866332292557\n",
      "0.10469906032085419\n",
      "0.052119649946689606\n",
      "0.041769251227378845\n",
      "0.065554678440094\n",
      "0.09948451817035675\n",
      "0.17056837677955627\n",
      "0.08245936781167984\n",
      "0.08461957424879074\n",
      "0.1264057755470276\n",
      "0.04030375927686691\n",
      "0.05840272828936577\n",
      "0.12588174641132355\n",
      "0.07462381571531296\n",
      "0.14622101187705994\n",
      "0.12516336143016815\n",
      "0.07050048559904099\n",
      "0.1298239678144455\n",
      "0.058189552277326584\n",
      "0.1102370098233223\n",
      "0.07155279815196991\n",
      "Epoch:  29 , Loss:  0.09054041005671025\n",
      "Training accuracy: 98%\n",
      "Test accuracy: 74%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    loss_acc = 0\n",
    "    model.train()\n",
    "    for data,label in dl_text:\n",
    "        data = data.cuda()\n",
    "        label = label.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(loss.item())\n",
    "        loss_acc +=loss.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch: ', epoch, ', Loss: ', loss_acc/(3200/64))\n",
    "    \n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data,label in dl_text:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        max_arg_output = torch.argmax(output, dim=1)\n",
    "        total_correct += int(torch.sum(max_arg_output == label))\n",
    "        total += data.shape[0]\n",
    "    print('Training accuracy: {:.0%}'.format(total_correct/total))\n",
    "\n",
    "    \n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data,label in dl_text_test:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        max_arg_output = torch.argmax(output, dim=1)\n",
    "        total_correct += int(torch.sum(max_arg_output == label))\n",
    "        total += data.shape[0]\n",
    "    print('Test accuracy: {:.0%}'.format(total_correct/total))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_inference(txt_inf):\n",
    "    encoded = roberta.encode(txt_inf)\n",
    "    encoded = torch.nn.functional.pad(encoded, pad=(0,400 - encoded.shape[0]), mode='constant', value=2)\n",
    "    roberta.eval()\n",
    "    model.eval()\n",
    "    output = model(encoded).softmax(dim=1)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoBERTaClassifier(\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=51200, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
